% mnras_template.tex
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}  % a4paper,

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
%\usepackage{newtxtext,newtxmath}
%\usepackage{lmodern}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
\usepackage{mathptmx}
%\usepackage{txfonts}


% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage{diagbox}

%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols
\usepackage{savesym}  % prevent symbol conflicts
\savesymbol{sf}
%\generate{%
%  \file{breqn.sty}{\nopreamble\from{breqn.dtx}{breqn.sty}}%
%}
%\usepackage{breqn} % automatic breaking equation 
%\usepackage{fancyvrb}
%\VerbatimFootnotes
\usepackage{cprotect}  % to allow verb in caption 
\DeclareMathOperator\erfc{erfc}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\cdf{cdf}
\DeclareMathOperator\sf{sf}
\DeclareMathOperator\isf{isf}
\DeclareMathOperator\ppf{ppf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[DRW fitting]{Using C\'el\'erit\'e to infer DRW parameters }

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[K. Suberlak et al.]{
Krzysztof Suberlak,$^{1}$\thanks{E-mail: suberlak@uw.edu}
\v{Z}eljko Ivezi\'c $^{1}$
\\
% List of institutions
$^{1}$Department of Astronomy, University of Washington, Seattle, WA, United States\\
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2017}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
A report to outline validation of C\'el\'erit\'e. We compare the tools used to fitting for $\tau$ and $SF_{\infty}$ to those of 
\cite{kozlowski2017a}and \cite{macleod2011}. To do so we reproduce some of experiments they conducted, and evaluate whether they can be mutually consistent. 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:1}

Quasars exhibit stochastic variability with characteristic timescales of hundreds of days (Kelly+2009,  Kozlowski+2010,2016, 2017 Macleod+2010,2011,2012, Zu+2011,2013,2016, Kasliwal+2015,2017 ). We employ  C\'el\'erit\'e \citep{foreman2017} , which allows to express any one -dimensional process as a Gaussian Process.  Gaussian Process is defined by covariance and mean. Covariance parameters are often called hyperparameters. To find best-fit hyperparameters we optimize the marginal likelihood  (eq. 5.4, 5.8, Rassmussen\&Williams book). Since the marginal likelihood is the integral of  the product of likelihood and prior,  the logarithm of marginalized likelihood is the sum of the log-likelihood and log-prior (eq.2.28 Rassmussen\&Williams). 

In this report we summarize the tests that have been made to establish great usefulness of  C\'el\'erit\'e in modelling the DRW light curves as Gaussian Processes. We  first compare whether algorithms used to make mock DRW light curves are identical between MacLeod +2011 (which we use), and \cite{kozlowski2017a} (who challenges Macleod+2011 results, and whose results we reproduce). Then we describe how  a choice of boundaries and priors affects the results of best-fit hyperparameters with  C\'el\'erit\'e. [[ We then compare C\'el\'erit\'e  to another well-tested tool (used by \cite{kozlowski2016a}, \cite{zu2011}, etc.) - JAVELIN ]].  We then reproduce results of \cite{macleod2011} Fig.15, and \cite{kozlowski2017a} Fig.2 .  	We aim to answer the following questions: 
\begin{enumerate}
\item are the tools for fitting tau and SFinf equivalent?
\item can we reproduce Chelsea`s Fig 15?
\item can we reproduce Kozlowski`s plot? 
\item are their plots mutually consistent, given our analysis? 
\item can we reproduce best-fit tau and SFinf obtained using light curve
     fitting with the SF approach? 
\end{enumerate}




\section{Simulating DRW}
\label{sec:simDRW}

We simulate damped random walk light curves by drawing  points from a Gaussian distribution, for which mean and standard deviation are re-calculated at each timestep. Given an input of observation times $t$,   $SF_{\infty}$ - the asymptotic value of the structure function, mean magnitude  $\langle y \rangle$, and the damping timescale $\tau$, we start at time $t_{0}$ and signal at that time is equal to the mean$y_{0} = \langle y \rangle$. The timestep is   
$\Delta t_{i} = t_{i+1} - t_{i}$.  Given  the signal at time $t_{i}$: $y_{i}$,  and $\Delta t_{i}$,  the signal at next time step $y_{i+1}$ is drawn from  $\mathcal{N}(loc, stdev)$, where : 

\begin{equation}
loc = y_{i} e ^ { - r  }  + \langle y \rangle \left( 1 - e ^{ - r }\right)
\end{equation}

and 

\begin{equation}
stdev^{2} =  0.5  \, \mathrm{SF}_{\infty}^{2} \left( 1 - e ^{  - 2 r  }  \right)
\end{equation}

with  $r = \Delta t_{i} / \tau$.   Here we followed eq. A4 and A5 in \cite{kelly2009}, as well as Sec. 2.2 in \cite{macleod2010}.  To this ideal light curve signal we add photometric noise $\mathcal{N}(0,n_{i})$, where $n_{i}$ is the observational photometric noise.  It is equivalent to  Kozlowski+2017 formulation , who also starts with the signal $s_{i}$ , drawing at each time step light curve points from  a Gaussian distribution with dispersion  $stdev$ and mean $loc$, subsequently adding the mean $\langle y \rangle$ and Gaussian noise (see Eq. (2) of \cite{kozlowski2017a}).


Apart from $\tau$ and $SF_{\infty}$, we choose $N_{pts}$ - at how many points to sample the simulated DRW process, and the length of baseline $T = l \cdot \tau$.  In this formalism the baseline multiplicity $l$  is equivalent to $1 / \rho$ where  $\rho = \tau / T $  (Kozlowski+2017). We can sample the baseline either at regular intervals of $\Delta t$, or at random $N_{pts}$. One sets the other - given  $\Delta t$,  we find $N_{pts}$ as the nearest integer to $t_{max} - t_{min} / \Delta t $. 


\section{DRW as Gaussian Process}
DRW is a stochastic process defined by the covariance matrix  

\begin{equation}
S_{ij} = \sigma^{2} \exp{ \left(   -  \Delta t_{ij} / \tau \right)}
\end{equation}
( see Kozlowski+2010 eq. 1, Kozlowski+2017 eq. 1,  MacLeod+2011 eq.1, Zu+2013 eq. 3 , etc ).  A scatter of magnitude difference plotted as a function of time lag $\Delta t_{ij} $ is called the Structure Function (SF).  SF for the Damped Random Walk is described by  :

\begin{equation}
SF(\Delta t_{ij} ) = SF_{\infty} \left( 1 - e^{-|\Delta t_{ij} /\tau|} \right) ^{1/2}
\end{equation}

For large $\Delta t_{ij} $ , we have    
\[ \lim_{\Delta t_{ij} \gg \tau} e^{-|\Delta t_{ij} /\tau| }= 1 \]
so that: \[   SF(\Delta t_{ij} \to \infty)\xrightarrow[]{} SF_{\infty} \].  Following MacLeod+2011,  we define the driving amplitude for shot-term variability as :

\begin{equation}
\hat{\sigma} = \sigma \sqrt{2 / \tau}
\end{equation} 

We can relate $SF_{\infty}$ to $\sigma$ and $\hat{\sigma}$ : 

\begin{equation}
SF_{\infty} = \hat{\sigma} \sqrt{\tau} = \sigma \sqrt{2}
\end{equation} 

thus $SF_{\infty}$ is just a scaled version of $\sigma$. 

Another often used combination of hyperparameters is called $K$ (as in MacLeod+2011) : 

\begin{equation}
K = \tau \sqrt{SF_{\infty} }= \tau \sqrt{\sigma} 2^{1/4} 
\end{equation}.  

In the $\log{\sigma}$ - $\log{\tau}$ space, lines of constant $K$ or $\hat{\sigma}$ are perpendicular to each other.  This is because, if we take $\log{\hat{\sigma}}$, and rearrange, we have  : 

\begin{equation}
\log{\sigma} = \frac{1}{2} \log{\tau} + \log{\hat{\sigma}} - \frac{1}{2} \log{2}
\label{eq:sigma_hat_line}
\end{equation}

and from  $\log{K}$ : 

\begin{equation}
\log{\sigma} = -2 \log{\tau} + \log{K} - \frac{1}{2} \log{2}
\label{eq:K_line}
\end{equation}

These equations denote lines $y = ax + b$,  and the slope of one is the  inverse reciprocal of another, which proves that they are orthogonal in that space (see Fig.~\ref{fig:lc_logL_arrows})

Covariance matrix, or kernel, is a function that defines similarity between two points. In general, a kernel is any function that maps $x$, $x'$ onto $\mathbb{R}$. Thus a covariance function is a specific type of a kernel.   A Gaussian process is defined by its covariance function and mean. To model light curves as DRW using Gaussian Process approach we use the Real Term kernel in Celerite : 

\begin{equation}
S_{ij} = a_{j} e^ {-c_{j} | t_{j} - t_{i}|}
\end{equation}

with parameters  $\log{(a)}$ and $\log{(c)}$.    It is clear that this is a DRW kernel if we substitute  $a_{j} \equiv \sigma^{2}$, and  $c_{j} \equiv \tau^{-1}$, so that $\log{(a)} = 2 \log ( \sigma)$, and $\log{(c)} = - \log (\tau)$.  By default there are no boundaries on parameter values, and there is no prior.  We find that imposing very liberal boundaries does not affect the result of fit but helps ensure computational stability. Thus we choose to limit $\sigma$ to between 0.01 and 1.0 mag, and $\tau$ to between 1 and 10000 days . 
Both \cite{macleod2011}  and \cite{kozlowski2017a} use Jeffreys prior \citep{jeffreys46} on $\tau$ and $\hat{\sigma}$ : $prior(\tau) = 1 / \tau$ ,  and $prior(\hat{\sigma}) = 1 / \hat{\sigma}$. 


In the Bayesian framework, we have in general : 

\begin{equation}
\mathrm{posterior} = \mathrm{likelihood} \cdot \mathrm{prior}
\end{equation}

so that :

\begin{equation}
-\log{(\mathrm{posterior})} = -\log{(\mathrm{likelihood})} - \log{(\mathrm{prior})}
\end{equation}

With  Jeffreys prior

\begin{equation}
\mathrm{prior} = \frac{1}{\hat{\sigma}}\frac{1}{\tau} = \frac{1}{\tau}\frac{1}{\sigma\sqrt{2/\tau}}=2^{-1/2}\tau^{-1/2}\sigma^{-1}
\end{equation}
i.e. 

\begin{equation}
\log{(\mathrm{prior})} = -\frac{1}{2}\log{(2)} + \frac{1}{2}\log{(c)}-\frac{\log{(a)}}{2}
\end{equation}

so that :

\begin{equation}
-\log{(\mathrm{posterior})} = -\log{(\mathrm{likelihood})} + \frac{1}{2}\left( \log{(2)} - \log{(c)} + \log{(a)} \right) 
\end{equation}


We estimate the error on the maximum likelihood estimate (MLE) in  the following way : if the MLE, based on the global maximum of the log-likelihood of the posterior distribition $\log{L}$, is $\hat{\theta}$, the standard deviation of $\hat{\theta}$ is the square root of variance $var(\theta)$. But the variance is the inverse of the Information  matrix :

\begin{equation}
var(\theta) = [I(\theta)]^{-1}
\end{equation}

which in turn is equal to the negative of the expected value of the Hessian matrix : 

\begin{equation}
[I(\theta)] = -E[H(\theta)]
\end{equation}

and the Hessian matrix is  the matrix of second derivatives of the likelihood with respect to the parameters : 

\begin{equation}
H(\theta) = \frac{\partial^{2}\log{L}}{\partial \theta \partial \theta }
\end{equation}


The parameters estimated in case of DRW fitting with Celerite were $\log{a}$ and $\log{c}$, so that the standard deviations describe the standard deviation in $\log{a}$ or $\log{c}$ :  $s_{\log{a}}$, $s_{\log{c}}$.  Since these are related to $\sigma$, $\tau$ via : $a \equiv \sigma^{2}$ and $c \equiv \tau^{1}$ (by definition), we translate the error on logarithms of $\sigma^{2}$ or $\tau^{1}$ to error  on $\sigma$, $\tau$ . To do that we use standard error propagation formulae : 

\begin{equation}
x = \log{p} , \\ 
s_{x} = \frac{s_{p}}{p} \\
\therefore s_{p}  = p \,s_{x}
\end{equation}

so that   $s_{a} = a\, s_{\log{a}} $ , $s_{c} = c\, s_{\log{c}}$, and 


\begin{equation}
x = p^{y} , \\ 
s_{x} = xy \frac{s_{p}}{p} \\
\therefore s_{p} = \frac{s_{x}p}{xy }
\end{equation}

so that $s_{\sigma} = s_{a} / 2 \sigma$, and $s_{\tau} = s_{c} \tau^{2}$





\section{Likelihood for GP}
Celerite efficiently evaluates the marginalized likelihood of the dataset under a Gaussian Process model with given kernel and hyperparameters. We optimize the  log-likelihood  for the best-fit hyperparameters with the stable   L-BFGS-B  \citep{lu1995}, \citep{zhu1997}  algorithm   using  \verb|scipy.optimize.minimize| \citep{jones2001} implementation.  We illustrate the shape of log-likelihood for a simulated light curve with parametes  {$\tau_{in}=100$ days, $\sigma_{in} = 0.2^{\mathrm{mag}}$, Gaussian noise of $0.001^{\mathrm{mag}}$, with length $20 \tau$, and regular sampling of $\Delta t = 1 $ day  }, and flat prior . See   Fig. ~\ref{fig:lc_logL_fit}  for the light curve and GP prediction, and Fig.~\ref{fig:lc_logL} for the the shape of log-likelihood evaluated for this data on the grid of hyperparameters $\sigma$, $\tau$. 

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/sim_lc_flat_prior_fit.png}
%\vskip -0.15in
\caption{A Celerite fit to a simulated light curve using a flat prior. }
\label{fig:lc_logL_fit}
\end{figure} 

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/sim_lc_loglike.png}
%\vskip -0.15in
\caption{The log likelihood for the simulated light curve on Fig.~\ref{fig:lc_logL_fit}. Black contours show 0.683, 0.955, 0.997 levels of the cumulative (integrated) posterior probability. }
\label{fig:lc_logL}
\end{figure} 





\begin{figure*}
\includegraphics[width=1.05\textwidth]{figs/sim_lc_logL_four_panels.png}
%\vskip -0.15in
\caption{For each pixel on the  $\sigma$ - $\tau$ grid we evaluated the log-likelihood value, $\log{L}$, shown on the bottom-right panel (same as Fig.~\ref{fig:lc_logL}). In addition, given these  $\sigma$ and $\tau$ we also evaluated $K$ and $\hat{\sigma}$, which enabled, given $\{ \sigma, \tau, \hat{\sigma}, K, \log{L} \}$, plotting $\log{L}$ in space of $K$-$\hat{\sigma}$, or any other parameter as a function of the other two. }
\label{fig:lc_logL_panels}
\end{figure*} 

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/sim_lc_log-log_K_sigma_hat_arrows.png}
%\vskip -0.15in
\caption{The log likelihood for the simulated light curve, plotted in  $\log{\sigma}$-$\log{\tau}$ space.  White gaps occur because originally the   $\sigma$ - $\tau$ grid on which we evaluated $\log{L}$ was linear, not logarithmic.  Black contours show 0.683, 0.955, 0.997 levels of the cumulative (integrated) posterior probability. Choosing the scale to be the same along both axes,  arrows that point along direction of constant $\hat{\sigma}$ or constant $K$ are perpendicular, as shown by Eqs.\ref{eq:sigma_hat_line} and \ref{eq:K_line}. }
\label{fig:lc_logL_arrows}
\end{figure} 


\section{Experimenting with number of points and baseline}
% or how we reproduced Macleod+2011 Fig. 15  : 
% Celerite_fit_simulated_data.ipynb 

% the simulation setup  : the knobs and whistles 
We simulate  light curves as described in Sec.~\ref{sec:simDRW}, using the same input parameters as \cite{macleod2011} : $\tau_{in} = 575 $ days, $SF_{\infty}= 0.2 $ mag, regular sampling interval of 10 days ,  $\sigma = SF_{\infty} / \sqrt{2} = 0.1414 $.  We start with 10 000 realizations of a very long (40 years)  and well-sampled  ($\Delta t = 10 days$) light curve.  We fit with Celerite , using bounds on $\sigma$ : [0.1 - 1.0] mag,  and bounds on $\tau$ : [1 - 10000 ] days. At each realization,  we select 1- , 3-, 10- year , full sections of the light curve. We fit at each light curve section length with   with both flat prior or Jeffrey's prior. 

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_Chelsea_multiyears_log_prior.png}
\caption{Sections of the 40-year light curve, fitted with the Jeffreys (log) prior. These sections are used to reproduce experiment from \citep{macleod2011}.  From top to bottom: 40-year, 10-year, 3-year, 1-year sections. The input is $\tau = 575$ days, $SF_{\infty} = 0.2$, so that $\sigma = 0.14$, homoscedastic error of 0.001 mag.  }
\label{fig:lc_sections}
\end{figure}


% the original figure  : how we have the same behavior 

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/MacLeod_2011_Fig_15_flat.png}
\caption{Distribution of results of 10000 iterations of DRW light curve , at each iteration fitting the full light curve, or its 1,3, or 10 year sections, shown with dotted, thick solid,  dashed, or thin solid lines, respectively. From top left panel, going clockwise, we display the ratio of each quantity derived from fitted $\tau$, $\sigma$ to the input values:  $\hat{\sigma}  = SF_{\infty}  / \sqrt{\tau}$,  $K = \tau \sqrt{SF_{\infty}}$, $SF_{\infty} = \sqrt{2} \sigma$, and $\tau$. We display the rms and bias calculated for each distribution ($rms \equiv  \sqrt{\langle x^{2}\rangle}$, $bias \equiv \langle x \rangle $, with the latter being the distribution mean). Note that, especially as seen on upper right and bottom panels, the longer the section of the light curve that we use, the smaller the bias. It is surprising that even for a well-sampled 40-year light curve the bias in all four quantities is nonzero, but the overall conclusions : that the result of DRW fit asymptotically converge to true values only for light curves much longer than $10 \tau$,  are similar to those of \citep{macleod2011}. }
\label{fig:macleod11_15}
\end{figure}


% two additional experiments : 

We also performed two controlled experiments : how changing the number of points, or changing the baseline, affects the results.  

% 1)  changing the number of points  : weird stuff happens  : 
     % inspecting the Structure Function .... 
With the former, we keep the light curve  baseline fixed at 40 years,  initially sampled by 1460 points ( that corresponds to the regular interval of 10 days : $40 * 365 / 10  = 1460$) - this is the same starting light curve as in the top panel of Fig.~\ref{fig:lc_sections}. 
We then increase the number of points by a factor $f \in {1,2,4,8}$. We illustrate that for flat prior on Fig.~\ref{fig:Npts_experiment}

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_Chelsea_Npts_test_flat_prior.png}
\caption{Experiment increasing the sampling density of the fixed baseline light curve. From top to bottom, we increase the initial number of points sampling the underlying process from 1460 to twice, four, and eight times more : second, third and fourth panels, respectively. }
\label{fig:Npts_experiment}
\end{figure}


We plot an equivalent measure as Fig.\ref{fig:macleod11_15}, detailing the results of this experiment - see Fig.~\ref{fig:macleod11_15_Npts}




\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/MacLeod_2011_Fig_15_N_pts_test_flat_prior.png}
\caption{Distribution of Celerite fit results of 1000 iterations of DRW light curve simulations in controlled number of points experiment. At  each iteration we make a realization of DRW light curve with input $\tau=575$ days, $SF_{\infty}=0.2$ mag, 40 -year baseline, sampled at regular intervals by $f \cdot 1460 $ points,  where $f \in {1,2,4,8}$.}
\label{fig:macleod11_15_Npts}
\end{figure}


% 2) changing the baseline : exactly as expected 
We also performed an experiment keeping the number of points per light curve fixed at N = 1460 , but extending the baseline by factor $f \in {1,2,4,8}$. All other parameters are as before ( $\tau = 575$ days, $SF_{\infty} = 0.2$ mag, $err = 0.001$ mag). We illustrate the light curves used on Fig.~\ref{fig:lc_length}.

% example how light curves change : 
\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_Chelsea_baseline_test_flat_prior.png}
\caption{Light curves used in baseline experiment: we freeze parameters   $\tau=575 $ days, $SF_{\infty}=0.2$ mag, $N$ (keep them fixed), and we extend the light curve baseline, starting from 40-years, and increasing it by a factor of $f \in {2,4,8}$, from top to bottom. Note : at each iteration, longer light curves are not a mere shifted copy of the base 40-year length light curve, but new realizations of the DRW process with the same $\tau$, $\sigma$, $N$, and different baseline.}
\label{fig:lc_length}
\end{figure}

% 

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/MacLeod_2011_Fig_15_baseline_test_flat_prior.png}
\caption{Distribution of Celerite fit results for 1000 iterations of DRW light curve simulations in controlled baseline experiment.  We fix input parameters $\tau$, $\sigma$, and $N$ of points,  but at each iteration we simulate four versions of the light curve  : with 40-year baseline, and those longer by a factor $f \in {2,4,8}$. We fit each baseline version with flat prior or Johnson prior. In each panel  on top of histograms of ratios of best-fit results to input value, we display the $rms$, $bias$, and light curve length in years. The histograms are color-coded by light curve baseline : 40 year (black dotted line), 80 year (thick solid blue line), 160 year (dashed magenta line), 320 year (thin solid red line). As we would expect, the longer the baseline, the less is the bias. The rms also decreases with increasing baseline, because results of fit become more centered on a single value.It is worth noting that even for very long baseline (320 years), we do observe nonzero bias.  }
\label{fig:macleod11_15_baseline}
\end{figure}


\section{Investigating the fitting bias as a function of prior, and sampling density}
We simulated 1000 light curves with $\tau=100$ days, $\sigma=0.2$ mag (so that $SF_{\infty}=0.2828$ mag), baseline of $20 \tau$, random sampling from  a uniform distribution with sampling interval of $\Delta t=5$ days,   homoscedastic error $0.001$ mag, $N=400$ points. To check the influence of choosing different priors, we fit these light curves  using flat or Johnson prior - see Fig.~\ref{fig:sim_lc_400}. See the same plot marginalized over the y-axis $\tau_{fit} / \tau_{input}$, or x-axis : $\sigma_{fit} / \sigma_{input}$. These show that there is a persistent bias even in well-sampled (400 points), long light curves with negligible photometric error. We explore on Fig.~\ref{fig:sim_lc_N_pts_flat} how this changes with increased sampling density (from 400 points to 3200 points) over the same baseline ($20 \tau$, with $\tau=100$ days) . This is achieved by an increase in sampling density $\Delta t$ by a factor $f \in {1,2,4,8}$. Thus $\Delta t$, the sampling interval, is changed as set ${5,2.5, 1.25, 0.625} $ days, and correspondingly the number of points N increases as ${400,800,1600,3200}$.


% these two figures use Zeljko's light curves, 
% but i've established in the notebooks 
% that my simulation and his are the same ... 

\begin{figure*}
\includegraphics[width=1.05\textwidth]{figs/DRW_priors_comp1.png}
%\vskip -0.15in
\caption{We plot results of fitting the simulated light curves with Celerite Real Term kernel, with flat (left), or Johnson prior (right). The cross shows the location of truth.  Both are offset, and Fig.~\ref{fig:sim_lc_400_marg} shows the marginalized version of that plot. We repeat the same experiment,  increasing the number of points tenfold to show the behavior of the bias. }
\label{fig:sim_lc_400}
\end{figure*} 


\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/sigma_tau_ratios_marginalized.png}
\caption{The marginalized version of Fig.~\ref{fig:sim_lc_400}. It shows that both with flat prior and Johnson prior,  there is a bias in fitting well-sampled (400 points), long (20 $\tau$), with virtually no error ($\sigma_{phot}=0.001 $ mag). We investigate how this changes with increased number of points on Figs.~\ref{fig:sim_lc_N_pts_flat} and \ref{fig:sim_lc_N_pts_log}}
\label{fig:sim_lc_400_marg}
\end{figure}

% two figures : for flat and for log (Johnson ) prior . 
% they show rms and bias,  

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_N_test_flatprior_.png}
\caption{A controlled experiment to probe how the sampling density affects the fit results with a large number (1000) of light curves, with increasing number of points ($N = f 400$, where $f \in {1,2,4,8}$), and thus increasing sampling density ($\Delta t = 5 /f $, f as before), while keeping  $\tau=100$ days, $\sigma=0.2$ mag  , baseline of $20 \tau$ unchanged. As we expect, both bias and rms decrease as a function of sampling density, although even for very generous sampling of $~12$ hrs we still do not have a distribution that is centered on the true value for this baseline. On this plot we used flat prior. }
\label{fig:sim_lc_N_pts_flat}
\end{figure}


\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_N_test_logprior_.png}
\caption{Same as Fig.~\ref{fig:sim_lc_N_pts_flat}, but with Johnson prior.}
\label{fig:sim_lc_N_pts_log}
\end{figure}

\section{Statistical wiggles:  exploring the shape of structure function}
To investigate the shape of structure function, we used the basic DRW light curve setup of $\tau=100$ days,  $\sigma=0.2$ mag, error = 0.001 mag, length = $20 \tau$, sampling $\Delta t = 5 $ days. We simulated 1000 light curves with identical parameters, and collecting points from  one (Fig.~\ref{fig:sf_1}), or an ensemble of  ten (Fig.~\ref{fig:sf_10}), hundred (Fig.~\ref{fig:sf_100}), and all thousand  (Fig. ~\ref{fig:sf_1000}) light curves, we plotted raw pairs of  $\Delta m_{i,j}, \Delta t_{i,j}$, as well as robust standard deviation in 200 bins of $\Delta t$.  Note that here $\Delta t_{i,j}$ denotes the time difference between $t_{i}$ and $t_{j}$. We find that the stochasticity of light curves prevents from using structure function for a single object, or even an ensemble with less than approximately 10 objects. 

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_SF_1.png}
\caption{Raw time and magnitude pairwise differences on the top panel, and on the bottom panel with blue dots the robust standard deviation of $\Delta m$ in 200 bins of $\Delta t$, with overplotted in orange dashed line the theoretical structure function (with $SF_{\infty}=0.2 \sqrt{2} $ mag, $\tau=100$ days). Data for only one light curve. }
\label{fig:sf_1}
\end{figure}

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_SF_10_LC.png}
\caption{Same as Fig.~\ref{fig:sf_1}, but combining data for 10 light curves.}
\label{fig:sf_10}
\end{figure}

\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_SF_100_LC.png}
\caption{Same as Fig.~\ref{fig:sf_1}, but combining data for 100 light curves.}
\label{fig:sf_100}
\end{figure}


% add figure with 1000 LC's ... 
\begin{figure}
\includegraphics[width=1.05\columnwidth]{figs/DRW_SF_1000_LC.png}
\caption{Same as Fig.~\ref{fig:sf_1}, but combining data for 1000 light curves.}
\label{fig:sf_1000}
\end{figure}


\section{Experiments with damping timescale retrieval }
% or how we reproduce Kozlowski+17 Fig.2 : 
% Celerite_Kozlowski_2017_plot.ipynb
% Celerite_Kozlowski_2017_simulate.ipynb 

Here we repeat the basic tenets of the experiment performed by \citep{kozlowski2017a}. He simulated 100-year long light curves, with cadence $\Delta t= 2 $ days, $\tau = 200 $days, $SF_{\infty} = 0.2$ mag , that were later degraded to the cadences of SDSS S82 or OGLE-III. Various sections of these light curves were used to test the bias caused by length of time series (baseline). 


% exactly the same simulation setup 
We simulated as the 'truth'  10 000 light curves with $\tau=575 $ days, $SF_{\infty}=0.2$ mag  (i.e.  $\sigma_{input} =SF_{\infty} /\sqrt{2} = 0.1414  $), baseline of $100 \tau = 57500$ days (157 years, but expressing the light curve length in terms of multiples of the underlying decorrelation timescale seems more informative). We chose the cadence of $\Delta t=1$ day, at regular intervals.

% https://cartesianproduct.wordpress.com/tag/kronecker-product/

Now we want to downsample the 'truth'. Kozlowski chose either N=60 points (SDSS-S82-like) or N=445 (OGLE-III-like). To explore a similar parameter space, we choose a grid of N $\in$ \{60,200,1000\} points. Given that, we then choose the baseline.  Because of minimum cadence of $\Delta t = 1$ day, there is a minimum light curve length we can obtain with a given  choice of number of points.   $T_{min}= N * \Delta t$, so that $T_{min} \in \{60,200,1000\} $ days. Following Kozlowski, we define the ratio of input time scale to  experiment length (baseline) as $\rho \equiv \tau / T$. Light curve length  $l = 1 / \rho = T / \tau$ corresponds to the baseline expressed in units of $\tau$. 

Given  $T_{min}$ , set by the number of points, the minimum light curve length we can choose is  $l_{min} = T_{min} / \tau$, all the way to to $  l_{max} = T_{max} / \tau = (100 * \tau )/ \tau = 100$.  

This means that, depending on the number of points $N$, we sample the  $\rho$ space from $\rho_{min} = 1 / l_{max} = \tau / T_{max} = \tau / 100 \tau = 1 / 100 $ , corresponding to the full baseline,  to  $\rho_{max} = 1 / l_{min} = \tau / T_{min} = \tau / N = 575 / N $. Thus, for  $N \in \{60,200,1000\}$,  the minimum value of $\rho$ is always $\rho_{min} = 0.001$, and the maximum values of $\rho$ are $\rho_{max} \in \{10,2,0.5\}$. 

With set to  $\tau_{in} = 575 days$, $\sigma_{in} = 0.2 / \sqrt(2)$  for all light curves,  the number of points per light curve defines $\rho_{min}$ and $\rho_{max}$. Using these as boundary values, we set up a logarithmic grid of $\rho$ , sampled at 100 values of $\rho$.  

Given the light curve length in terms of $\rho$ and number of points $N$ we explore the effect of  regular and  random sampling. Finally, following Koz{\l}owski, we introduce a random Gaussian noise, at one of three possible levels :  \{0.001, 0.01, 0.1 \} mag.  

Thus, given the 10 000 'true' light curves, we sample the 18-element phase space  : { Regular / Random sampling  } $\times$   \{ 60, 200, 1000 points\} $\times$ \{0.001, 0.01, 0.1 mag noise\} where $\times$   indicates the Cartesian product. 

For each combination of tuple (number of points, sampling type, noise level ) we span the grid of $\rho$, so that for each of the 10 000 light curves we select 100  sections varying in length. We illustrate the sampling procedure (in other words, the grid of $\rho$), on Fig.~\ref{fig:sampling}. 

It is worth mentioning that in total we performed 18 * 10 000 * 100  = 18 million  light curve fits with Celerite, and thanks to its optimized performance, it only took 6 hours on a Macbook laptop  (< 1 milisecond per lightcurve, which is great considering the overhead of file input / output to store the results and read light curves from independent text files).  
    

% illustrate sampling a lightcurve 
\begin{figure*}
\includegraphics[width=1.05\textwidth]{figs/Sampling_illustrate_regular_0-001_noise.png}
%\vskip -0.15in
\caption{We illustrate the process of selecting different sections of a light curve, depending on the desired number of points N, and length of section : $\rho$. Here we chose regular sampling, and negligible 0.001 mag noise, but the principle is exactly the same regardless of noise level or sampling procedure. From left to right, we sample $N \in {1000,200,60}$ points. Focusing on a single column, from top to bottom we sample on a logarithmic grid of $\rho$. The smallest $\rho$ is set by the maximum attainable light curve section, which corresponds to the full length $l = 100 \tau$, and since $\rho = 1 / l$, $\rho_{min} = 0.01$. The largest $\rho$ is related to the shortest possible light curve section conditional on the number of points chosen. Thus choosing $N \in {1000,200,60}$ days, the shortest possible sections are $l \in {1000,200,60}$ days given that we have adopted the $\Delta t = 1 $ day in 'true' light curve.}
\label{fig:sampling}
\end{figure*} 

 
% results, i.e. Fig2 , and how they depend on a combination 
% of noise,  Npts, sampling....

We compare the retrieved timescale to the input timescale, parametrized as a fraction of light curve (section) length. As the noise level increases, so does the spread in retrieved values of characteristic timescale, as seen on Fig.~\ref{fig:change_noise}.  Fig.~\ref{fig:change_N} illustrates  that the number of points does not appreciably affect the bias level.

\begin{figure*}
\includegraphics[width=1.05\textwidth]{figs/Fig_2_change_noise.png}
\caption{We plot the histogram of input to output $\rho$, and overplot the median of counts in bins of $\log_{10}{(\rho_{in})}$. The red dashed line marks the expected output in case of perfect fit.  We see  that indeed for light curves less than ~10 times the length of characteristic timescale the retrieved timescale is biased low. Increasing the number of points per lightcurve decreases the vertical scatter, i.e. decreases the rms of $\rho_{out}$ in bins of $\rho_{in}$.  We find that random vs regular sampling does not affect the plot morphology, thus we only show the random sampling.  On this plot, from left to right, we increase the photometric error from 0.001 to 0.1, keeping the number of points fixed at 1000. }
\label{fig:change_noise}
\end{figure*} 


\begin{figure*}
\includegraphics[width=1.05\textwidth]{figs/Fig_2_change_N.png}
\caption{Similarly to Fig.~\ref{fig:change_noise}, but keeping noise fixed at 0.001 mag level , random sampling , and decreasing from left to right the number of points from 1000 to 60. }
\label{fig:change_N}
\end{figure*} 

We also evaluate how the fractional "bias", i.e ratio 

\begin{equation}
\frac{\rho_{out}-\rho_{in}}{\rho_{in}}
\end{equation}

depends on $\rho_{in}$ -  the ratio of input timescale ($\tau=575 days$) to baseline ($T$, depends on the number of points).
Fig.~\ref{fig:fracbias_changeN} shows that the measured decorrelation timescales are biased low by 25 \%  as long as the light curve is at least 10 times longer than the true decorrelation timescale ($\rho_{in} < 0.1 $, or $\log_{10}(\rho_{in}) < -1.0$). 


\begin{figure*}
\includegraphics[width=1.05\textwidth]{figs/Fig_3_changeN_keepNoise-0001_sampling-random.png}
\caption{Histogram of the fractional bias of $\rho_{out}-\rho_{in} / \rho_{in}$, as a function of the input $\rho$ (=575 / N). From left to right we change N from 1000 to 60 which sets the maximum value of $\rho_{in}$. We fix photometric noise at 0.001 mag level and select random sampling. Overplotted is the median fractional bias (orange dots), the unbiased level (horizontal dotted line), and the value of $\rho_{in} = 0.1$ (vertical solid line). Note that regardless of the number of points sampling the lightcurve, the bias is less than 25\%  as long as  the length of light curve is at least ten times longer than the true decorrelation timescale ($\tau = 575 $days ).}
\label{fig:fracbias_changeN}
\end{figure*} 

Finally , on Fig.~\ref{fig:symmetry_changeN} we check the symmetry of $\sigma_{out} / \sigma_{in}$, to verify that the spread is symmetric , and not biased either way for shorter light curves. 


\begin{figure*}
\includegraphics[width=1.05\textwidth]{figs/Fig_4_changeN_keepNoise-0001_sampling-random.png}
\caption{Histogram of the 'measured' $\sigma_{out}$ to 'true' $\sigma_{in}$. As in Fig.~\ref{fig:fracbias_changeN}, we change the N points from left to right, keeping the sampling type (random) and photometric error (0.001 mag) unchanged. The orange circles mark the median y. We also overplot the horizontal dotted line at y=0, and vertical solid line at x=-1.0.}
\label{fig:symmetry_changeN}
\end{figure*} 


\section{Conclusions}
% or,  have we answered the questions posed initially ?  

We can confirm results of Kozlowski+2017 regarding an inherent biased introduced in fitting the DRW process. We find that even without any prior (flat prior), the results can be reproduced - the $\rho_{out}$ is biased low for light curves shorter than $\approx 10 \tau$.  However,  the bias is not large for light curves even of length only twice the input characteristic time scale. We argue that it is necessary to quantify the 'bias' and 'goodness' of  theoretically possible performance of  fitting the DRW process with available software. Our choice of software - Celerite - did not introduce any significant bias as compared to the tools used by Kozlowski+2017 (since the internal workings of Celerite are similar to Press-Rybicki-Hewitt method - the reason that Celerite is fast is the same that afforded the PRH method to be so quick). Indeed, the shape of likelihood used with Celerite, that better constrains $\hat{\sigma}$ than $\tau$ or $\sigma$  individually, matches the shape of likelihood in Kozlowski (PRH) method.  We propose the measure of percentage departure from the 'truth' as the measure for the goodness of fit,  and we choose  to consider results within $10 \%$ of the true input value of $\tau$ to be 'good'. 

However,  given that $\hat{\sigma}$ may be better constrained than $\tau$ purely due to the likelihood shape,  we suggest that perhaps even for light curves that are too short to estimate 'well' the input timescale, we are able to estimate the asymptotic structure function value. This helps to select quasars from Stars, since even if the timescale cannot be well estimated,  the amplitude of structure function ( driven by $\sigma$) ,  can help distinguish quasars from background noise, since their amplitude of variation is larger. 

We argue that DRW fitting , and recovering the amplitude of variability within the damped random walk model, can help distinguish  quasar light curves from background noise (or noisy stellar measurements) better than other statistical measures (such as chi2 per degree of freedom, standard deviation,  or rms of the light curve).  

To show that ,  we  simulated  DRW light curves in range of tau, and range of sigma.  With the same sampling, we also simulated white noise, that would be reproducing the  noisy measurements (is there any better model for random noise of measurement  for SDSS  or LSST  ? ). For each light curve, fitted with Celerite with DRW model,  we recover $\tau$ and $\sigma$.  We also calculate $\mathcal{P}$ parameters : $\chi^{2}_{DOF}$, $\chi^{2}_{R}$, standard deviation,  rms.  For each set of input tau, sigma,  we have $N$ realizations, and for each $i-th$ realization there are parameters  $\mathcal{P}_{i}$ . We plot a histogram of $\mathcal{P}$ for each value of input $\tau$, $\sigma$. We record the mean and median averaged over many realizations. We plot that as  a two-dimensional histogram as  $\log{(\rho_{in})}$  vs each parameter $\mathcal{P}$, overplotting the mean and median (along y-axis). 


 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

% The best way to enter references is to use BibTeX:

\bibliographystyle{mnras}
\bibliography{references} % if your bibtex file is called references.bib

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
